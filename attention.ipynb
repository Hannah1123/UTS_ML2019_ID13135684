{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNQYRml+Fk0Hix6krPLxO3Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hannah1123/UTS_ML2019_ID13135684/blob/master/attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ynxV_2X1-c4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math, copy, time\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn\n",
        "seaborn.set_context(context=\"talk\")\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szkdtG3vyd9V",
        "colab_type": "text"
      },
      "source": [
        "#encoder-decoder 框架"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZ_rOh9J2PgR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "  \"\"\"\n",
        "  标准Encoder-Decoder架构 enbed: 嵌入 src: source tgt: target\n",
        "  nn.module:神经网络模块化接口\n",
        "  \"\"\"\n",
        "  def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
        "    super(EncoderDecoder, self).__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.src_embed = src_embed\n",
        "    self.tgt_embed = tgt_embed\n",
        "    self.generator = generator\n",
        "  \n",
        "  def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "    \"\"\"\n",
        "    接收并处理屏蔽的source和target序列。\n",
        "    \"\"\"\n",
        "    return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
        "  \n",
        "  def encode(self, src, src_mask):\n",
        "    return self.encoder(self.src_embed(src), src_mask)\n",
        "    \n",
        "  def decode(self, memory, src_mask, tgt, tgt_mask):\n",
        "    return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZNtJjbrVXtO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "  \"\"\"\n",
        "  定义标准线性和softmax\n",
        "  \"\"\"\n",
        "  def __init__(self, d_model, vocab):\n",
        "    super(Generator, self).__init__()\n",
        "    self.proj = nn.Linear(d_model, vocab)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return F.log_softmax(self.proj(x), dim= -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngi-_ZfLXH_h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "encoder 由N=6个完全相同的层堆叠。\n",
        "1、copy.copy 浅拷贝 只拷贝父对象，不会拷贝对象的内部的子对象。\n",
        "2、copy.deepcopy 深拷贝 拷贝对象及其子对象: 即将被复制对象完全再复制一遍作为独立的新个体单独存在。所以改变原有被复制对象不会对已经复制出来的新对象产生影响。 \n",
        "\n",
        "生成N个完全相同的层：\n",
        "\"\"\"\n",
        "def clones(module, N):\n",
        "  return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ta8thVxCIpk4",
        "colab_type": "text"
      },
      "source": [
        "#Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68Z-DT8uRZVt",
        "colab_type": "text"
      },
      "source": [
        "主encoder是一个N层的堆叠  \n",
        "为什么clones放在class encoder外面？ ：Encoder和Decoder都会用到这个clones  \n",
        "LayerNorm 归一化？"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XGRWw8GRSwE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, layer, N):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.layers = clones(layer, N)\n",
        "    self.norm = LayerNorm(layer.size)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    \"\"\"\n",
        "    依次使input（和mask）通过每个layer。\n",
        "    \"\"\"\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, mask)\n",
        "    return self.norm(x)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcbDXCwadoXB",
        "colab_type": "text"
      },
      "source": [
        " residual connection： 残差连接，输入和输出相加，作为下一层的输入。  \n",
        " dense connection： 把所有层都与其他所有层相连，也就是说N层的网络，会有N*(N-1)/2个连接。用于减少计算量。densenet就同时做了两件事情，一是将网络中的每一层都直接与其前面层相连，提高特征的利用率；二是把网络的每一层设计得很窄，也就是卷积的输出通道数通常很小，只有几十，该层学习非常少的特征图并与输入concat使用。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFqSfqnLWrWG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, features, eps=1e-6):\n",
        "    super(LayerNorm, self).__init__()\n",
        "    self.a_2 = nn.Parameter(torch.ones(features))\n",
        "    self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "    self.eps = eps\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(-1, keepdim = True)\n",
        "    std = x.std(-1, keepdim = True)\n",
        "    return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZNtQJs6iOQD",
        "colab_type": "text"
      },
      "source": [
        "每个子层的输出为LayerNorm(x + Sublayer(x))，其中Sublayer(x）为每个子层完成的function。  \n",
        "We apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized.  \n",
        "为了方便残差连接，所有子层和嵌入层的输出维度为d=512"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vn0Wd7DNjOho",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SublayerConnection(nn.Module):\n",
        "  def __init__(self, size, dropout):\n",
        "    \"\"\"\n",
        "    一个残差连接之后接一个Layer norm 归一化\n",
        "    \"\"\"\n",
        "    super(SuperlayerConnection, self).__init__()\n",
        "    self.norm = LayerNorm(size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, sublayer):\n",
        "    \"\"\"应用 残差连接 到所有有相同size的子层\"\"\"\n",
        "    return x + self.dropout(sublayer(self.norm(x)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2B-ItXhkzcU",
        "colab_type": "text"
      },
      "source": [
        "Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed- forward network.  \n",
        "encoder由selt-attn 和 feed forward（前馈） 组成  \n",
        "  \n",
        "\n",
        "  lambda: 函数表达式\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMuYraDUn4m0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "    self.self_attn = self_attn\n",
        "    self.feed_forward = feed_forward\n",
        "    self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
        "    self.size = size\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    \"\"\"根据figure 1 的左边来连接\"\"\"\n",
        "    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
        "    return self.sublayer[1](x, self.feed_forward)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5rRps20IxlX",
        "colab_type": "text"
      },
      "source": [
        "#Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ov18-owICI47",
        "colab_type": "text"
      },
      "source": [
        "Decoder 也是由 N=6 个完全相同的层堆叠  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yt_z65OcCGxL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, layer, N):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.layers = clones(layer, N)\n",
        "    self.norm = LayerNorm(layer.size)\n",
        "\n",
        "  def forward(self, x, memory, src_mask, tgt_mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, memory, src_mask, tgt_mask)\n",
        "    return self.norm(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GG8nrlFMJOxJ",
        "colab_type": "text"
      },
      "source": [
        "in addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization  \n",
        "加入第三个子层 对encoder的堆叠 multi-head 注意。和encoder一样，每个子层有残差连接 并加上layer norm 归一化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zk5-8XmfJ1hS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "    self.size = size\n",
        "    self.self_attn = self_attn\n",
        "    self.src_sttn = src_attn\n",
        "    self.feed_forward = feed_forward\n",
        "    self.sublayer = clones(SublayerConnection(size,dropout), 3)\n",
        "\n",
        "  def forward(self, x, memory, src_mask, tgt_mask):\n",
        "    m = memory\n",
        "    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
        "    x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
        "    return self.sublayer[2](x, self.feed_forward)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TGy_YZgNp8R",
        "colab_type": "text"
      },
      "source": [
        "We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions.  \n",
        "确保位置i的输出只受已知位置（位置i之前）的输出的影响，加入一个子层mask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hN8EItPFOEY7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def subsequent_mask(size):\n",
        "    \"确定后续的位置.\"\n",
        "    attn_shape = (1, size, size)\n",
        "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
        "    return torch.from_numpy(subsequent_mask) == 0\n",
        "    #torch.from_numpy:转换格式"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJBChs3Jn8Li",
        "colab_type": "code",
        "outputId": "45316b94-a9cd-4ea9-a679-3c9e0bac8df1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737
        }
      },
      "source": [
        "np.ones((1, 20, 20))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "         1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "         1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "         1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "         1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "         1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "         1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "         1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "         1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "         1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "         1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "         1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "         1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "         1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "         1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "         1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "         1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "         1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "         1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "         1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "         1., 1., 1., 1.]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e3lKHwroZgb",
        "colab_type": "code",
        "outputId": "8988230e-0a70-402d-837a-61e2f0dd8d75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737
        }
      },
      "source": [
        "np.triu(np.ones((1,20,20)), k=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "         1., 1., 1., 1.],\n",
              "        [0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "         1., 1., 1., 1.],\n",
              "        [0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "         1., 1., 1., 1.],\n",
              "        [0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "         1., 1., 1., 1.],\n",
              "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "         1., 1., 1., 1.],\n",
              "        [0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "         1., 1., 1., 1.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "         1., 1., 1., 1.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "         1., 1., 1., 1.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
              "         1., 1., 1., 1.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
              "         1., 1., 1., 1.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
              "         1., 1., 1., 1.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
              "         1., 1., 1., 1.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
              "         1., 1., 1., 1.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
              "         1., 1., 1., 1.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
              "         1., 1., 1., 1.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         1., 1., 1., 1.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 1., 1., 1.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 1., 1.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 1.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0.]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1slTFr0oqd8",
        "colab_type": "code",
        "outputId": "46b4ae95-e164-45c5-f729-8509c01bff65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        }
      },
      "source": [
        "np.triu(np.ones((1,20,20)), k=1).astype('uint8')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]],\n",
              "      dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfwjO8R_jQ2D",
        "colab_type": "text"
      },
      "source": [
        "figure语法说明\n",
        "\n",
        "figure(num=None, figsize=None, dpi=None, facecolor=None, edgecolor=None, frameon=True)\n",
        "*   num: 图像编号或名称，数字为编号 ，字符串为名称\n",
        "*   figsize: 指定figure的宽和高，单位为英寸；  \n",
        "*   dpi 参数指定绘图对象的分辨率，即每英寸多少个像素，缺省值为80\n",
        "*   facecolor: 背景颜色\n",
        "*   edgecolor: 边框颜色\n",
        "*   frameon: 是否显示边框\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15emTTP4R9kE",
        "colab_type": "code",
        "outputId": "ac123864-d4a8-424c-cb34-38df41283398",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "source": [
        "plt.figure(figsize=(5,5))\n",
        "plt.show()\n",
        "plt.imshow(subsequent_mask(20)[0])\n",
        "None"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQkAAAEHCAYAAAC9YrMUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyZCQVPo/hvhZP0vDFCMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GP0MB59-y-jQ",
        "colab_type": "text"
      },
      "source": [
        "#attention  \n",
        "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.  \n",
        "attention机制由 query，key，value三个向量组成。输出是values的加权总和，权重由query和相应的key确定"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCZ1NCRY_Vit",
        "colab_type": "text"
      },
      "source": [
        "We call our particular attention “Scaled Dot-Product Attention”. The input consists of queries and keys of dimension $d_k$, and values of dimension $d_v$. We compute the dot products of the query with all keys, divide each by $\\sqrt{d_k}$, and apply a softmax function to obtain the weights on the values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTufyKUZ-A9h",
        "colab_type": "text"
      },
      "source": [
        "将当前词作为搜索的query，去和句子中所有词（包含该词本身）的key去匹配，看看相关度有多高  \n",
        "我们分别得到了Q1 与 K1,K2 的点乘积，然后我们进行尺度缩放与softmax归一化,得到attention score  \n",
        "在用这些attention score与value vector相乘，得到加权的向量"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7thUtnALzAFH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def attention(quary, key, value, mask=None, dropout=None):\n",
        "  d_k = query.size(-1)\n",
        "  scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "  #matmul：高维向量乘法  transpose：转置 为什么要转置？？\n",
        "  if mask is not None:\n",
        "    scores = scores.masked_fill(mask == 0, -1e9)\n",
        "  p_attn = F.softmax(scores, dim = -1) \n",
        "  # dim=0表示按列计算；dim=1表示按行计算，dim=-1,即,以最后一个维度作为一维向量计算softmax\n",
        "  if dropout is not None:\n",
        "    p_attn = dropout(p_attn)\n",
        "  return torch.matmul(p_atten, value), p_attn\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1X1sZaJcOdUz",
        "colab_type": "text"
      },
      "source": [
        "而multihead就是我们可以有不同的Q,K,V表示，最后再将其结果结合起来"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "es0SXhMwL68O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "  def __init__(self, h, d_model, dropout=0.1):\n",
        "    super(MultiHeadedAttention, self).__init__()\n",
        "    assert d_model % h == 0 \n",
        "    #假设 d_v 永远等于 d_k ???\n",
        "    self.d_k = d_model // h  #减少维度使总计算量相当于single-head用全维度计算 512/8=64维\n",
        "    self.h = h   #取h=8\n",
        "    self.attn = None\n",
        "    self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "  def forward(self, query, key, value, mask=None):\n",
        "    if mask is not None:\n",
        "      #同一个mask 用于所有h head\n",
        "      mask = mask.unsqueeze(1) #在1的位置加入维度为1的维度，如原来为（4，3），就变为（4，1，3） \n",
        "    nbatches = query.size(0)  #0维上的元素个数\n",
        "  \n",
        "    \"1）线性投影 从d_model到 h*d_k\"\n",
        "    query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) for l, x in zip(self.linears, (query, key, value))]\n",
        "                 # view（）函数：重塑张量维形状(nbatches, -1, self.h, self.d_k)，（-1表示未知，由计算机自己算），之后 转置（1，2）\n",
        "\n",
        "    \"2）应用attention到投影向量上\"\n",
        "    x, self.attn = attention(query, key, value, mask=mask, dropout = self.dropout)\n",
        "\n",
        "    \"3）\"Concat\" using a view and apply a final linear. \"\n",
        "    x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h*self.d_k)\n",
        "    return self.linears[-1](x)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5kW9aDoO-mu",
        "colab_type": "text"
      },
      "source": [
        "unsqueeze用法：  \n",
        "torch.squeeze() 这个函数主要对数据的维度进行压缩，去掉维数为1的的维度，比如是一行或者一列这种，一个一行三列（1,3）的数去掉第一个维数为一的维度之后就变成（3）行。squeeze(a)就是将a中所有为1的维度删掉。不为1的维度没有影响。a.squeeze(N) 就是去掉a中指定的维数为一的维度。还有一种形式就是b=torch.squeeze(a，N) a中去掉指定的定的维数为一的维度。\n",
        "\n",
        "再看torch.unsqueeze()这个函数主要是对数据维度进行扩充。给指定位置加上维数为一的维度，比如原本有个三行的数据（3），在0的位置加了一维就变成一行三列（1,3）。a.squeeze(N) 就是在a中指定位置N加上一个维数为1的维度。还有一种形式就是b=torch.squeeze(a，N) a就是在a中指定位置N加上一个维数为1的维度  \n",
        "torch.squeeze(input, dim=None, out=None) → Tensor  \n",
        "dim指定维度\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWkyeNT3Wwto",
        "colab_type": "text"
      },
      "source": [
        "**向量的点乘为什么要使用投影？**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezLu6HMqXEBM",
        "colab_type": "text"
      },
      "source": [
        "#Applications of Attention in our Model  \n",
        " 1）“encoder-decoder attention” layers, Q从之前的decoder得来，而K和V由encoder得来。这允许decoder中的每个位置都参与输入序列中的所有位置。这模仿了seq2seq模型中的典型encoder-decoder注意机制。  \n",
        " 2) The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\n",
        "\n",
        "3) Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot- product attention by masking out (setting to $-\\infty$) all values in the input of the softmax which correspond to illegal connections."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NL63_ukCNoCq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}