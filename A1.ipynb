{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hannah1123/UTS_ML2019_ID13135684/blob/master/A1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOYDzv_4wZP-",
        "colab_type": "text"
      },
      "source": [
        "#Review on 'Gradient-Based Learning Applied to Document Recognition'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2jbflpcwmqU",
        "colab_type": "text"
      },
      "source": [
        "#Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUdzxlMIwqUc",
        "colab_type": "text"
      },
      "source": [
        "#Content \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_TdkYve78lX",
        "colab_type": "text"
      },
      "source": [
        "Rumelhart and Hinton[1] firstly came up with the algorithm of back propagation in 1985 which made the training of neural networks feasible. A few years later, LeCun[2] applied back propagation to realize the recognition of handwritten zip code. At the same time, though convolutional neural networks had not be named yet, they were firstly implemented.   \n",
        "This paper is focus on handwritten character recognition using a new classifier called LeNet-5. This reaserch has implemented real-life document recognition system which consist of multiple modules including feature extraction, segmentation, recognition and language modeling.  \n",
        "One of the major challenge in handwritting recognition is to extract the separate characters from a word or a sentence. It is difficult and extremely costly to creat a labeled database which consist of incorrectly segmented characters. Convolutional Neural Networks are described in this paper which were designed to learn to seperate out features directly from pixel images. The implement of convolutional neural network is helpful for emliminating the need for hand-crafted feature extrators. The results of this network has been shown the advantages in training a system at word or sentence level. At the same time, the use of convolutional neural networks significantly save the computational cost.  \n",
        "Convolutional neural networks combine local receptive field, shared weights and sub-sampling.One of the typical convolutional network is LeNet-5 which is presented below.  \n",
        "![LeNet-5](https://github.com/Hannah1123/UTS_ML2019_ID13135684/blob/master/1566745651(1).png)   \n",
        "As for the recognition, this research compares several methods for isolated handwritten digit recognition including Linear Classifier, Pairwise Linear Classifier, Baseline Nearest Neighbor Classifier, PCA, One-hidden Layer Fully Connected Multilayer Neural Network, Two-hidden Layer Fully Connected Multilayer Neural Network, LeNet-1, LeNet-4, Boosted LeNet-4,TDC, SVM.  \n",
        "This research also presents the elimination of segmentation heuristics by using Apace-Displacement Neural Network approach. Heuristic Over-Segmentation is used to generate a lot of hypothese in parallel in order to postpone desisions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qce_FDS1xDjM",
        "colab_type": "text"
      },
      "source": [
        "#Innovation \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FI0MayWYyS8T",
        "colab_type": "text"
      },
      "source": [
        "The background at the time of the work is that Hubel and Wiesel shows that by connecting an electrode to a neuron, they can listen to the response of the neuron to a bar of light. They came up with a new understanding of how V1 cerebral cortical neurons work.[3] In 1980, Kunihiko Fukishima proposed a neural network model for mechanism of visual pattern recognition called neocognitron.[4]\n",
        "Rumelhart and Hinton firstly came up with the algorithm of back propagation in 1985 which made the training of neural networks feasible. A few years later, LeCun applied back propagation to realize the recognition of handwritten zip code. At the same time, though convolutional neural networks had not be named yet, they were firstly implemented.   \n",
        "The creative idea is a new architecture of convolutional neural networks. LeNet-5 consist of 7 layers: three Convolutions(C1, C3, C5), two Subsampling(S2, S4), one Full connection(F6) and a Gaussian connections(Output layer).\n",
        "The convolutions are comprises of filters which involve trainable parameters(weights). \n",
        "The Subsampling is to reduce the spatial size of the outcome from convolutions to reduce computation to quarter in the network. At the sanme time, the process of subsampling has the feature of invariance including translation invariance, rotation invariance and scale invariance.  \n",
        "The methods adopting neural network has been shown to have better performance. They not only run much faster but also require much less space.  \n",
        "This research successfully applied Grandient-Based Learning to Convolutional Neural Networks which has been shown to learn appropriate features from examples. As a result, Grandient-Based Learning has been proved that it is useful for learning in large system. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdtXXqObyWTY",
        "colab_type": "text"
      },
      "source": [
        "# Technical quality\n",
        "rate如何评价论文中工作的技术质量(要考虑已完成的工作)。200字  \n",
        "定义了CNN的基本结构\n",
        "CNN 的鼻祖\n",
        "提出新的架构 using。。。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIZDelO6iz2Z",
        "colab_type": "text"
      },
      "source": [
        "The technical development if of high quality. this research compares several methods for isolated handwritten digit recognition including Linear Classifier, Pairwise Linear Classifier, Baseline Nearest Neighbor Classifier, PCA, One-hidden Layer Fully Connected Multilayer Neural Network, Two-hidden Layer Fully Connected Multilayer Neural Network, LeNet-1, LeNet-4, Boosted LeNet-4,TDC, SVM. The authors supported their theory using ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yF0V2JbzyfC-",
        "colab_type": "text"
      },
      "source": [
        "# Application and X-factor\n",
        "该技术是否符合其应用域，还可以用于什么application domain？给一些该research发展的建议。该文章的内容能否在课堂上激起讨论的火花？有什么interest 发现 在该文章中？200字  \n",
        "图像识别 语音识别 人脸识别\n",
        "梯度消失\n",
        "复杂问题处理不理想\n",
        "计算跟不上"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyZ_oCF_iio8",
        "colab_type": "text"
      },
      "source": [
        "I find the proposal in the paper promising. ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9Cw-PKx2QNt",
        "colab_type": "text"
      },
      "source": [
        "# Presentation\n",
        "该文章的presentation的质量如何？rate：how easy it was to follow the argument in the paper; presentation风格; depth of the argument; and clarity of\n",
        "the presentation.#100字  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gndkJxHpikDO",
        "colab_type": "text"
      },
      "source": [
        "The overall strucutre is clear. I found reading is easy / difficult. The paper could have been more attractive if the authors had organised ... / provided ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9xgNyuk2yx-",
        "colab_type": "text"
      },
      "source": [
        "# References\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pM03wepY3hkz",
        "colab_type": "text"
      },
      "source": [
        "# Draft and Experiment Area"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zu7cFOned-XK",
        "colab_type": "text"
      },
      "source": [
        "First impression\n",
        "\n",
        "What is my chosen paper to read?  \n",
        " **Gradient-Based Learning Applied to Document\n",
        "Recognition**  \n",
        "\n",
        "What type of the main contribution the paper has made? \n",
        "A theory or proposition (revealing something, from unknown to known)\n",
        "A method or algorithm (inventing a technique, from undoable to doable)\n",
        "Before reading the main body of the paper, write down your first impression obtained from its abstract and short introduction.  \n",
        "**basic of CNN**   \n",
        "\n",
        "Why does the paper attract you, such as, How it surprised you? Why do you think it addresses an important topic that will be helpful in your future study of machine learning?  \n",
        "**CNN is one of the basic of machine learning**\n",
        "\n",
        "Read the paper abstract and introduction, list here all the notions that you don't know the precise meaning. If you think you have completed your list, compare the list with people around you who have chosen the same or a similar paper.  \n",
        " \n",
        "\n",
        "\n",
        "*   **multilayer nueral networks**\n",
        "*   **backpropagation algorithm**\n",
        "\n",
        "*   **gradient-based learning**\n",
        "*   **language modeling**\n",
        "\n",
        "*   **consvolutional neural networks**\n",
        "*   **feature extraction**\n",
        "\n",
        "*   **LeNet-5**\n",
        "*   **loss function**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "(During the next 7 days) Re-consider the central problem of the paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76oDJ4_bD-2z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        },
        "outputId": "3eab2118-f414-4088-a9f4-5f025555b62d"
      },
      "source": [
        "#卷积：tf.nn.conv2d\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "input = tf.Variable(tf.random_normal([1, 3, 3, 5]))\n",
        "filter = tf.Variable(tf.random_normal([1, 1, 5, 2]))\n",
        "op = tf.nn.conv2d(input, filter, strides=[1, 1, 1, 1], padding='VALID')\n",
        "sess = tf.InteractiveSession()\n",
        "tf.global_variables_initializer().run()\n",
        "print(sess.run(op))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[[ 1.8813169  -1.5202918 ]\n",
            "   [ 2.2968774   0.0779307 ]\n",
            "   [-0.5835189   0.07772313]]\n",
            "\n",
            "  [[ 0.43748137  2.6572256 ]\n",
            "   [-0.45102707  2.6266615 ]\n",
            "   [-0.6183824   1.7114183 ]]\n",
            "\n",
            "  [[ 3.8748915   0.7497179 ]\n",
            "   [ 0.8550094   0.01759055]\n",
            "   [ 0.3192418  -0.01004475]]]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gW4h-GZyEB3k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "outputId": "7eed79f5-77fe-4a23-e32c-69262c1286aa"
      },
      "source": [
        "#池化tf.nn.max_pool\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "a = tf.constant([\n",
        "    [[1.0, 2.0, 3.0, 4.0],\n",
        "     [5.0, 6.0, 7.0, 8.0],\n",
        "     [8.0, 7.0, 6.0, 5.0],\n",
        "     [4.0, 3.0, 2.0, 1.0]],\n",
        "    [[4.0, 3.0, 2.0, 1.0],\n",
        "     [8.0, 7.0, 6.0, 5.0],\n",
        "     [1.0, 2.0, 3.0, 4.0],\n",
        "     [5.0, 6.0, 7.0, 8.0]]\n",
        "])\n",
        "\n",
        "a = tf.reshape(a, [1, 4, 4, 2])\n",
        "\n",
        "pooling = tf.nn.max_pool(a, ksize=[1, 2, 2, 1], strides=[1, 1, 1, 1], padding='VALID')\n",
        "with tf.Session() as sess:\n",
        "    image = sess.run(a)\n",
        "    print(image)\n",
        "    result = sess.run(pooling)\n",
        "    print(result)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[[1. 2.]\n",
            "   [3. 4.]\n",
            "   [5. 6.]\n",
            "   [7. 8.]]\n",
            "\n",
            "  [[8. 7.]\n",
            "   [6. 5.]\n",
            "   [4. 3.]\n",
            "   [2. 1.]]\n",
            "\n",
            "  [[4. 3.]\n",
            "   [2. 1.]\n",
            "   [8. 7.]\n",
            "   [6. 5.]]\n",
            "\n",
            "  [[1. 2.]\n",
            "   [3. 4.]\n",
            "   [5. 6.]\n",
            "   [7. 8.]]]]\n",
            "[[[[8. 7.]\n",
            "   [6. 6.]\n",
            "   [7. 8.]]\n",
            "\n",
            "  [[8. 7.]\n",
            "   [8. 7.]\n",
            "   [8. 7.]]\n",
            "\n",
            "  [[4. 4.]\n",
            "   [8. 7.]\n",
            "   [8. 8.]]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDacVaknAXaM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "91d4d730-96d2-4970-e622-47b4b719c7ad"
      },
      "source": [
        "import tensorflow as tf  \n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "  \n",
        "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)  \n",
        "  \n",
        "sess = tf.InteractiveSession()  \n",
        "  \n",
        "# 训练数据\n",
        "x = tf.placeholder(\"float\", shape=[None, 784])  \n",
        "# 训练标签数据\n",
        "y_ = tf.placeholder(\"float\", shape=[None, 10])  \n",
        "# 把x更改为4维张量\n",
        "x_image0 = tf.reshape(x, [-1, 28, 28, 1])\n",
        "\n",
        "#28*28->32*32\n",
        "x_image = tf.pad(x_image0,[[0,0],[2,2],[2,2],[0,0]])\n",
        "\n",
        "def weight_variable(shape):\n",
        "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def bias_variable(shape):\n",
        "    initial = tf.constant(0.1, shape=shape)\n",
        "    return tf.Variable(initial)\n",
        "  \n",
        "#卷积\n",
        "def conv2d(x,W):\n",
        "  return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='VALID')  \n",
        "\n",
        "#池化\n",
        "def max_pool_2x2(x):\n",
        "  return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID')\n",
        "\n",
        "\n",
        "def lenet5_layer(layer,weight,bias):\n",
        "  w_conv=weight_variable(weight)\n",
        "  b_conv=bias_variable(bias)\n",
        "  h_conv=conv2d(layer,w_conv)+b_conv\n",
        "  return max_pool_2x2(h_conv)\n",
        "\n",
        "#卷积C1+池化S2:->6*28*28 ->6*14*14\n",
        "layer1=lenet5_layer(x_image,[5,5,1,6],[6])\n",
        "\n",
        "#卷积C3+池化S4： ->16*10*10 ->16*5*5\n",
        "layer2=lenet5_layer(layer1,[5,5,6,16],[16])\n",
        "\n",
        "#卷积C5：-> 120*1*1\n",
        "w_conv3=weight_variable([5,5,16,120])\n",
        "b_conv3=bias_variable([120])\n",
        "\n",
        "layer3=conv2d(layer2,w_conv3)+b_conv3\n",
        "layer3=tf.reshape(layer3,[-1,120])\n",
        "\"\"\"\n",
        "\n",
        "W_conv1 = weight_variable([5, 5, 1, 6])\n",
        "b_conv1 = bias_variable([6])\n",
        "h_conv1=conv2d(x_image,W_conv1)+b_conv1\n",
        "h_pool1 = max_pool_2x2(h_conv1)\n",
        "\"\"\"\n",
        "\n",
        "#F6: \n",
        "def dense_layer(layer,weight,bias):\n",
        "  w_fc=weight_variable(weight)\n",
        "  b_fc=bias_variable(bias)\n",
        "  return tf.matmul(layer,w_fc)+b_fc\n",
        "\n",
        "con_layer1=dense_layer(layer3,[120,84],[84])\n",
        "\n",
        "#output\n",
        "con_layer2=dense_layer(con_layer1,[84,10],[10])\n",
        "keep_prob = tf.placeholder(tf.float32) \n",
        "y_conv=tf.nn.softmax(tf.nn.dropout(con_layer2,keep_prob))#减少过拟合\n",
        "\n",
        "  \n",
        "  \n",
        "#定义交叉熵损失函数  \n",
        "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[1]))  \n",
        "  \n",
        "#选择优化器，并让优化器最小化损失函数/收敛, 反向传播  \n",
        "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)  \n",
        "  \n",
        "# tf.argmax()返回的是某一维度上其数据最大所在的索引值，在这里即代表预测值和真实值  \n",
        "# 判断预测值y和真实值y_中最大数的索引是否一致，y的值为1-10概率  \n",
        "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))  \n",
        "  \n",
        "# 用平均值来统计测试准确率  \n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))  \n",
        "  \n",
        "#开始训练  \n",
        "sess.run(tf.global_variables_initializer())  \n",
        "for i in range(1000):  \n",
        "    batch = mnist.train.next_batch(100)  \n",
        "    if i%100 == 0:  \n",
        "        train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1], keep_prob: 1.0}) #评估阶段不使用Dropout  \n",
        "        print(\"step %d, training accuracy %g\" % (i, train_accuracy))  \n",
        "    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5}) #训练阶段使用50%的Dropout  \n",
        "  \n",
        "  \n",
        "#在测试数据上测试准确率  \n",
        "print(\"test accuracy %g\" % accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 0, training accuracy 0.14\n",
            "step 100, training accuracy 0.61\n",
            "step 200, training accuracy 0.78\n",
            "step 300, training accuracy 0.85\n",
            "step 400, training accuracy 0.85\n",
            "step 500, training accuracy 0.85\n",
            "step 600, training accuracy 0.89\n",
            "step 700, training accuracy 0.86\n",
            "step 800, training accuracy 0.93\n",
            "step 900, training accuracy 0.96\n",
            "test accuracy 0.9299\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}